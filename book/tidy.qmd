# Reshaping and Tidy Data

There's a [whole paper](https://vita.had.co.nz/papers/tidy-data.pdf) by Hadley Wickham about tidy data but it's in a PDF so you're probably not going to read it. Here's the paper's definition of tidy data:

1. Each variable forms a column.
2. Each observation forms a row.
3. Each type of observational unit forms a table.

As Mr Wickham notes, this is [Codd's 3rd Normal Form](https://en.wikipedia.org/wiki/Third_normal_form)
but in statspeak rather than databasespeak.

The meaning of "variable" and "observation" depend on what we're studying, so tidy data is a concept that you mostly learn through experience and âœ¨vibesâœ¨.

Now we'll explore what tidy data looks like for an NBA results dataset, where we want to investigate if home team advantage is real.

## Get the data

``` {python}
from pathlib import Path
import polars as pl
import pandas as pd

pl.Config.set_tbl_rows(5)
pd.options.display.max_rows = 5

nba_dir = Path("../data/nba/")

column_names = {
    "Date": "date",
    "Visitor/Neutral": "away_team",
    "PTS": "away_points",
    "Home/Neutral": "home_team",
    "PTS.1": "home_points",
}

if not nba_dir.exists():
    nba_dir.mkdir()
    for month in (
        "october",
        "november",
        "december",
        "january",
        "february",
        "march",
        "april",
        "may",
        "june",
    ):
        # In practice we would do more data cleaning here, and save to parquet not CSV.
        # But we save messy data here so we can clean it later for pedagogical purposes.
        url = f"http://www.basketball-reference.com/leagues/NBA_2016_games-{month}.html"
        tables = pd.read_html(url)
        raw = (
            pl.from_pandas(tables[0].query("Date != 'Playoffs'"))
            .rename(column_names)
            .select(column_names.values())
        )
        raw.write_csv(nba_dir / f"{month}.csv")

nba_glob = nba_dir / "*.csv"
pl.scan_csv(nba_glob).head().collect()
```

## Cleaning ðŸ§¹

Nothing super interesting here:

::: {.panel-tabset}
## Polars

``` {python}
games_pl = (
    pl.scan_csv(nba_glob)
    .filter(
        ~pl.fold(
            acc=True,
            f=lambda acc, s: acc & s.is_null(),
            exprs=pl.all(),
        )
    )
    .with_row_count("game_id")
    .with_column(
        pl.col("date").str.strptime(pl.Date, "%a, %b %d, %Y"),
    )
)
games_pl.head().collect()
```

## Pandas

``` {python}
games_pd = (
    pl.read_csv(nba_glob)
    .to_pandas()
    .dropna(thresh=4)
    .assign(date=lambda x: pd.to_datetime(x["date"], format="%a, %b %d, %Y"))
    .set_index("date", append=True)
    .rename_axis(["game_id", "date"])
    .sort_index()
)
games_pd.head()
```
:::

One thing we haven't seen already is [`pl.fold`](https://pola-rs.github.io/polars-book/user-guide/dsl/folds.html).
This is for fast horizontal operations, and it's what the Polars docs recommend for dropping a row where every column is null. In Pandas we do that via `.dropna(thresh=num_cols)`.

## Pivot and Melt

I recently came across someone who was doing advanced quantitative research in Python yet had never heard of the Pandas `.pivot` method. I shudder to imagine the code he must have written in the absence of this knowledge, so here's a simple explanation of `pivot`ing and `melt`ing, lest anyone else suffer in ignorance.

### Pivot

Suppose you have a dataframe that looks like this:

``` {python}
#| code-fold: true
from datetime import date
prices = pl.DataFrame({
    "date": [*[date(2020, 1, 1)]*4, *[date(2020, 1, 2)]*4, *[date(2020, 1, 3)]*4],
    "ticker": [*["AAPL", "TSLA", "MSFT", "NFLX"]*3],
    "price": [100, 200, 300, 400, 110, 220, 330, 420, 105, 210, 315, 440],
})
prices
```

In both Polars and Pandas you can call `df.pivot(index="date", values="price", columns="ticker")`
to get a dataframe that looks like this:

``` {python}
#| code-fold: true
pivoted = prices.pivot(index="date", values="price", columns="ticker")
pivoted
```

As you can see, `.pivot` creates a dataframe where the columns are the unique labels from one column ("ticker"), alongside the index column ("date"). The values for the non-index columns are taken from the corresponding rows of the `values` column ("price").

If our dataframe had multiple prices for the same ticker on the same date, we would use the `aggregate_fn` parameter of the `.pivot` method, e.g.: `prices.pivot(..., aggregate_fn="mean")`. Pivoting with an aggregate function gives us similar behaviour to what Excel calls "pivot tables".

### Melt

Melt is the inverse of pivot. While pivot takes us from *long* data to *wide* data, melt goes from wide to long.
If we call `.melt(id_vars="date", value_name="price")` on our pivoted dataframe we get our original dataframe back:

``` {python}
pivoted.melt(id_vars="date", value_name="price")
```

If you're still reading

``` {python}
tidy_pl = (
    games_pl.melt(
        id_vars=["game_id", "date"],
        value_vars=["away_team", "home_team"],
        value_name="team",
    )
    .sort("date")
    .with_column(pl.col("date").alias("rest").diff().over("team").dt.days() - 1)
    .drop_nulls("rest")
    .collect()
)
tidy_pl.head()
```